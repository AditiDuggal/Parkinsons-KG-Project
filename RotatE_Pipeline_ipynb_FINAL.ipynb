{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditiDuggal/Parkinson-s-Triples/blob/main/RotatE_Pipeline_ipynb_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SPARQLWrapper requests pandas"
      ],
      "metadata": {
        "id": "-zHyhlktq18Q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwXlfE-Tyqfr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/pykeen/pykeen.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from pykeen.triples import TriplesFactory"
      ],
      "metadata": {
        "id": "d2n1fSh-sDbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging, sys, time, json, csv, re\n",
        "#set up logging\n",
        "LOG_DIR = Path(\"./wd_debug\")\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout),\n",
        "              logging.FileHandler(LOG_DIR / \"wikidata_debug.log\", encoding=\"utf-8\")],\n",
        ")\n",
        "logger = logging.getLogger(\"wikidata-debug\")"
      ],
      "metadata": {
        "id": "18MkXLjJp5VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual: Dict[str, str] = {\n",
        "  \"AD\": \"Alzheimer's disease\",\n",
        "  \"5-HT\": \"Serotonin\",\n",
        "  \"6-OHDA\": \"6-hydroxydopamine\",\n",
        "  \"MPP+\": \"1-methyl-4-phenylpyridinium\",\n",
        "  \"A. muciniphila\": \"Akkermansia muciniphila\",\n",
        "  \"alpha-synuclein\": \"Alpha-synuclein\",\n",
        "  \"butyrate\": \"Butyrate\",\n",
        "  \"gut microbiota\": \"Gut microbiota\",\n",
        "  \"PD\": \"Parkinson's disease\",\n",
        "  \"Lewy bodies\": \"Lewy body\",\n",
        "  \"PCD\": \"Programmed cell death\",\n",
        "  \"6-OHDA\": \"6-hydroxydopamine\",\n",
        "  \"FMT\": \"Fecal microbiota transplantation\",\n",
        "  \"IBD\": \"Inflammatory bowel disease\",\n",
        "  \"Proteins\": \"Protein\",\n",
        "  \"Probiotics\": \"Probiotic\",\n",
        "  \"UC\": \"Ulcerative colitis\",\n",
        "  \"ATP\": \"Adenosine triphosphate\",\n",
        "  \"MPTP\": \"1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine\",\n",
        "  \"TNF-α\": \"Tumor necrosis factor alpha\",\n",
        "  \"SCFA\": \"Short-chain fatty acid\",\n",
        "  \"NO\": \"Nitric oxide\",\n",
        "  \"ROS\": \"Reactive oxygen species\",\n",
        "  \"the UPDRS\": \"Unified Parkinson's disease rating scale\",\n",
        "  \"gut microbiota dysbiosis\": \"Dybiosis\",\n",
        "  \"BMS\": \"Burning Mouth Syndrome\",\n",
        "  \"SLPI\": \"Secretory Leukocyte Protease Inhibitor\",\n",
        "  \"OTU\": \"Operational Taxonomic Unit\",\n",
        "  \"GI\": \"Gastrointestinal Tract\",\n",
        "  \"IBD\": \"Irritable Bowel Disease\",\n",
        "  \"EGCs\": \"Electrocardiogram\",\n",
        "  \"plantarum PFM\": \" Lactobacillus plantarum\",\n",
        "  \"CRC\": \"Colorectal Cancer\",\n",
        "  \"putamenal DA\": \"Putaminal Dopamine\",\n",
        "  \"UC\": \"ulcerative colitis\",\n",
        "  \"LBs\": \"Lewy Bodies\",\n",
        "  \"PCD\": \"Primary Ciliary Dyskinesia\",\n",
        "}\n",
        "\n",
        "def clean_label(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    s = re.sub(r'\\s+', ' ', s)                       # collapse spaces\n",
        "    s = re.sub(r'\\s-\\s', '-', s)                     # fix spaced hyphens\n",
        "    s = re.sub(r'^[,;:\\-\\u2013\\u2014]+\\s*', '', s)   # leading punctuation\n",
        "    s = re.sub(r'\\(\\s*aged [^)]+\\)', '', s, flags=re.I)\n",
        "    s = re.sub(r'(kg\\s*/\\s*m2|%|ppm|mg/L|<|>|≤|≥)', '', s, flags=re.I)\n",
        "    # corpus fixes\n",
        "    s = s.replace('Constip ation', 'Constipation')\n",
        "    s = s.replace('Mehtyl', 'Methyl')\n",
        "    s = s.replace('A . muciniphila', 'Akkermansia muciniphila')\n",
        "    s = s.replace('alpha - synuclein', 'alpha-synuclein')\n",
        "    s = s.replace('biﬁ', 'bifi').replace('Biﬁ', 'Bifi')\n",
        "    return s.strip()\n",
        "\n",
        "def is_entity_like(s: str) -> bool:\n",
        "    if not s or len(s) < 3: return False\n",
        "    if re.search(r'^\\d+(\\.\\d+)?\\s*$', s): return False             # numbers\n",
        "    if re.search(r'^(Acknowledgments|Introduction)\\b', s, re.I): return False\n",
        "    if re.search(r'^(Fig|Figure|Table|Supp(?:lement)?|S\\d+)\\b', s): return False\n",
        "    if re.search(r'^[A-Z]?\\d+[A-Z]?(?:\\s*-\\s*[A-Z])?\\b', s): return False  # 7C-F etc.\n",
        "    if len(s.split()) > 8: return False                            # likely a caption/sentence\n",
        "    return True\n",
        "\n",
        "def prepare_for_lookup(raw: str, manual_map: Dict[str,str]) -> List[str]:\n",
        "    s = clean_label(raw)\n",
        "    if not is_entity_like(s):\n",
        "        return []\n",
        "    if s in manual_map:\n",
        "        return [manual_map[s]]  # manual takes priority\n",
        "    # surface variants\n",
        "    variants = {s, s.replace(' - ', '-'), s.replace('- ', '-'), s.replace(' -', '-')}\n",
        "    return [v for v in variants if v]"
      ],
      "metadata": {
        "id": "iGm1kQVNqn63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIKIDATA_SEARCH = \"https://www.wikidata.org/w/api.php\"\n",
        "CACHE_PATH = Path(\"qid_cache.json\")\n",
        "ATTEMPTS_CSV = LOG_DIR / \"lookup_attempts.csv\"\n",
        "\n",
        "if not ATTEMPTS_CSV.exists():\n",
        "    with open(ATTEMPTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"raw\",\"cleaned\",\"entity_like\",\"used_manual\",\n",
        "                                \"candidate\",\"http_status\",\"qid\",\"error\"])\n",
        "\n",
        "def load_cache() -> Dict[str, Optional[str]]:\n",
        "    if CACHE_PATH.exists():\n",
        "        return json.loads(CACHE_PATH.read_text(encoding=\"utf-8\"))\n",
        "    return {}\n",
        "\n",
        "def save_cache(cache: Dict[str, Optional[str]]) -> None:\n",
        "    CACHE_PATH.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "rDdeYKjKqsI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wd_search_label_once(label: str, language: str = \"en\") -> Tuple[Optional[str], Optional[int], Optional[str]]:\n",
        "    headers = {\"User-Agent\": \"LailahKG/1.0 (mailto:your_email@example.com)\"}  # put your email\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": label,\n",
        "        \"language\": language,\n",
        "        \"format\": \"json\",\n",
        "        \"type\": \"item\",\n",
        "        \"limit\": 1,\n",
        "        \"strict\": 0,  # softer matching like your other function\n",
        "    }\n",
        "    backoff = 0.5\n",
        "    for attempt in range(5):\n",
        "        try:\n",
        "            r = requests.get(WIKIDATA_SEARCH, params=params, headers=headers, timeout=20)\n",
        "            status = r.status_code\n",
        "            if status == 200:\n",
        "                hits = r.json().get(\"search\", [])\n",
        "                return (hits[0][\"id\"] if hits else None, status, None)\n",
        "            if status in (429, 502, 503, 504):\n",
        "                time.sleep(backoff); backoff *= 2\n",
        "                continue\n",
        "            return (None, status, f\"HTTP {status}\")\n",
        "        except Exception as e:\n",
        "            err = str(e)\n",
        "            time.sleep(backoff); backoff *= 2\n",
        "            if attempt == 4:\n",
        "                return (None, None, err)\n",
        "    return (None, None, \"max retries\")"
      ],
      "metadata": {
        "id": "_qSbv-vmqu-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_labels_to_qids(labels: List[str],\n",
        "                       language: str = \"en\",\n",
        "                       sleep_s: float = 0.15,\n",
        "                       retry_null_cache: bool = True) -> Dict[str, Optional[str]]:\n",
        "    cache = load_cache()\n",
        "    out: Dict[str, Optional[str]] = {}\n",
        "\n",
        "    for raw in labels:\n",
        "        # respect cache unless we are retrying previous nulls\n",
        "        if raw in cache and (cache[raw] is not None or not retry_null_cache):\n",
        "            out[raw] = cache[raw]\n",
        "            continue\n",
        "\n",
        "        cleaned = clean_label(raw)\n",
        "        entity_like = is_entity_like(cleaned)\n",
        "        used_manual = int(cleaned in manual)\n",
        "        candidates = prepare_for_lookup(raw, manual)\n",
        "\n",
        "        qid: Optional[str] = None\n",
        "        if not candidates:\n",
        "            with open(ATTEMPTS_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                csv.writer(f).writerow([raw, cleaned, entity_like, used_manual, \"\", \"\", \"\", \"filtered_out_or_no_candidates\"])\n",
        "            logger.debug(f\"SKIP: {raw!r} -> {cleaned!r} (not entity-like)\")\n",
        "        else:\n",
        "            for cand in candidates:\n",
        "                qid, http_status, err = wd_search_label_once(cand, language=language)\n",
        "                with open(ATTEMPTS_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                    csv.writer(f).writerow([raw, cleaned, entity_like, used_manual, cand,\n",
        "                                            http_status if http_status is not None else \"\",\n",
        "                                            qid if qid else \"\", err if err else \"\"])\n",
        "                logger.info(f\"Lookup raw={raw!r} cleaned={cleaned!r} cand={cand!r} status={http_status} qid={qid} err={err}\")\n",
        "                if qid:\n",
        "                    break\n",
        "\n",
        "        cache[raw] = qid\n",
        "        out[raw] = qid\n",
        "        time.sleep(sleep_s)\n",
        "\n",
        "    save_cache(cache)\n",
        "    return out"
      ],
      "metadata": {
        "id": "BOi9WTOpqzi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wikidata_edges(qids: List[str]) -> List[Tuple[str, str, str]]:\n",
        "    props = {\"P31\",\"P279\",\"P361\",\"P460\",\"P171\",\"P105\"}\n",
        "    endpoint = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    triples: List[Tuple[str, str, str]] = []\n",
        "    BATCH = 50\n",
        "    batched = [qids[i:i+BATCH] for i in range(0, len(qids), BATCH)]\n",
        "\n",
        "    for batch in batched:\n",
        "        values = \" \".join([f\"wd:{qid}\" for qid in batch])\n",
        "        query = f\"\"\"\n",
        "        SELECT ?s ?p ?o WHERE {{\n",
        "          VALUES ?s {{ {values} }}\n",
        "          ?s ?p ?o .\n",
        "          FILTER(STRSTARTS(STR(?p), \"http://www.wikidata.org/prop/direct/\"))\n",
        "          FILTER(STRSTARTS(STR(?o), \"http://www.wikidata.org/entity/\"))\n",
        "        }}\n",
        "        \"\"\"\n",
        "        endpoint.setQuery(query)\n",
        "        endpoint.setReturnFormat(JSON)\n",
        "        try:\n",
        "            data = endpoint.query().convert()\n",
        "            for b in data[\"results\"][\"bindings\"]:\n",
        "                p_iri = b[\"p\"][\"value\"]\n",
        "                pid = p_iri.rsplit(\"/\", 1)[-1]   # P##\n",
        "                if pid not in props:\n",
        "                    continue\n",
        "                s_q = b[\"s\"][\"value\"].rsplit(\"/\", 1)[-1]\n",
        "                o_q = b[\"o\"][\"value\"].rsplit(\"/\", 1)[-1]\n",
        "                triples.append((f\"wd:{s_q}\", f\"wdt:{pid}\", f\"wd:{o_q}\"))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching wikidata edges for batch: {e}\")\n",
        "            # Continue with the next batch even if one fails\n",
        "            pass\n",
        "        time.sleep(1) # Add a delay of 1 second between batches\n",
        "\n",
        "\n",
        "    triples = list(set(triples))\n",
        "    return triples"
      ],
      "metadata": {
        "id": "1zKKSPVKq4No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Combined Cleaned Triples CSV - Sheet1.tsv', sep='\\t', header=None)\n",
        "assert df.shape[1] == 3, \"data.tsv must have exactly 3 tab-separated columns (head, relation, tail).\""
      ],
      "metadata": {
        "id": "sfsUc_oFrWYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S2X1I84NecjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\"Butyrate\", \"Alpha-synuclein\", \"Akkermansia muciniphila\", \"Gut microbiota\"]\n",
        "mini = map_labels_to_qids(tests, retry_null_cache=True)\n",
        "for t in tests:\n",
        "    print(t, \"->\", mini.get(t))"
      ],
      "metadata": {
        "id": "UE5v6n2kq7u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_labels = sorted(set(df[0].tolist()) | set(df[2].tolist()))\n",
        "label2qid = map_labels_to_qids(entity_labels)"
      ],
      "metadata": {
        "id": "bmg2l1K70iaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1a89e75"
      },
      "source": [
        "label2qid_df = pd.DataFrame(list(label2qid.items()), columns=['Label', 'Wikidata QID'])\n",
        "\n",
        "display(label2qid_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_label2qid_from_df(map_df: pd.DataFrame,\n",
        "                            label_col: str = \"Label\",\n",
        "                            qid_col: str = \"Wikidata QID\") -> Dict[str, Optional[str]]:\n",
        "    out: Dict[str, Optional[str]] = {}\n",
        "    for _, row in map_df[[label_col, qid_col]].iterrows():\n",
        "        label = str(row[label_col]).strip()\n",
        "        qid = row[qid_col]\n",
        "        # Accept QIDs that are either \"Q123\" or \"wd:Q123\" or numeric-r like \"Q12345\"\n",
        "        if pd.isna(qid) or (isinstance(qid, str) and qid.strip().lower() in {\"none\", \"nan\", \"\"}):\n",
        "            out[label] = None\n",
        "        else:\n",
        "            qid_str = str(qid).strip()\n",
        "            # strip any \"wd:\" prefix so values are canonical \"Q123\" strings or keep if already \"Q...\"\n",
        "            if qid_str.startswith(\"wd:\"):\n",
        "                qid_str = qid_str.split(\":\", 1)[1]\n",
        "            out[label] = qid_str\n",
        "    return out"
      ],
      "metadata": {
        "id": "4bRSLlFLwL8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remap_entities_to_qids(df: pd.DataFrame,\n",
        "                           label2qid: Dict[str, Optional[str]],\n",
        "                           head_col_idx: int = 0,\n",
        "                           tail_col_idx: int = 2) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Replace heads/tails using label2qid mapping.\n",
        "    If a mapping exists for the exact label, use wd:QID.\n",
        "    If the cell already looks like 'wd:Q123' or 'Q123', preserve/normalize to 'wd:Q123'.\n",
        "    Otherwise leave the original string unchanged.\n",
        "    \"\"\"\n",
        "    def normalize_cell(x):\n",
        "        if pd.isna(x):\n",
        "            return x\n",
        "        s = str(x).strip()\n",
        "        if s.startswith(\"wd:Q\") or s.startswith(\"wdt:Q\"):\n",
        "            return s  # keep\n",
        "        if s.startswith(\"Q\") and s[1:].isdigit():\n",
        "            return f\"wd:{s}\"\n",
        "        # try direct mapping\n",
        "        q = label2qid.get(s)\n",
        "        if q:\n",
        "            return f\"wd:{q}\"\n",
        "        # fallback: try a lowercase/cleaned exact-match attempt\n",
        "        q2 = label2qid.get(s.lower())\n",
        "        if q2:\n",
        "            return f\"wd:{q2}\"\n",
        "        # no mapping found -> return original label\n",
        "        return s\n",
        "\n",
        "    out = df.copy()\n",
        "    out.iloc[:, head_col_idx] = out.iloc[:, head_col_idx].map(normalize_cell)\n",
        "    out.iloc[:, tail_col_idx] = out.iloc[:, tail_col_idx].map(normalize_cell)\n",
        "    return out"
      ],
      "metadata": {
        "id": "0WoEycfmwyoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2qid = build_label2qid_from_df(label2qid_df, label_col=\"Label\", qid_col=\"Wikidata QID\")\n",
        "df_remapped = remap_entities_to_qids(df, label2qid)"
      ],
      "metadata": {
        "id": "EmNkrB9YwjPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resolved_qids = set()\n",
        "for val in label2qid.values():\n",
        "  if val:\n",
        "    resolved_qids.add(val)\n",
        "resolved_qids = list(resolved_qids)\n",
        "print(resolved_qids)"
      ],
      "metadata": {
        "id": "ChFfU1oPw6R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikidata_edges = fetch_wikidata_edges(resolved_qids)"
      ],
      "metadata": {
        "id": "8bOtMJh_xN6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wikidata_edges)"
      ],
      "metadata": {
        "id": "8RnfcexNxcke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_np = df_remapped.to_numpy(str)\n",
        "wd_np = np.array(wikidata_edges, dtype=str) if wikidata_edges else np.empty((0,3), dtype=str)"
      ],
      "metadata": {
        "id": "taH286mFyEn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_map = {\"wdt:P31\":\"instance_of\",\"wdt:P279\":\"subclass_of\",\"wdt:P361\":\"part_of\",\n",
        "           \"wdt:P460\":\"same_as\",\"wdt:P171\":\"parent_taxon\",\"wdt:P105\":\"taxon_rank\"}\n",
        "if wd_np.size:\n",
        "    wd_np_relabeled = wd_np.copy()\n",
        "    for i in range(wd_np_relabeled.shape[0]):\n",
        "        if wd_np_relabeled[i,1] in rel_map:\n",
        "            wd_np_relabeled[i,1] = rel_map[wd_np_relabeled[i,1]]\n",
        "    wd_np = wd_np_relabeled"
      ],
      "metadata": {
        "id": "ForUB5xwyZnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_triples = np.concatenate([orig_np, wd_np], axis=0)\n",
        "mask = np.char.startswith(all_triples[:, 0], \"wd:\") & np.char.startswith(all_triples[:, 2], \"wd:\")\n",
        "qid_only_triples = all_triples[mask]\n",
        "\n",
        "print(f\"Original triples: {all_triples.shape[0]}\")\n",
        "print(f\"QID-only triples: {qid_only_triples.shape[0]}\")"
      ],
      "metadata": {
        "id": "E507fZph55Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_triples_factory = TriplesFactory.from_labeled_triples(qid_only_triples)\n",
        "training_factory, testing_factory, validation_factory = all_triples_factory.split(\n",
        "    ratios=(0.8, 0.1, 0.1),\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "PERuipnwynZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtMgAk8azYI8"
      },
      "outputs": [],
      "source": [
        "import pykeen\n",
        "pykeen.env()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pykeen.pipeline import pipeline\n",
        "\n",
        "result = pipeline(\n",
        "    model='RotatE',\n",
        "    training=training_factory,\n",
        "    testing=testing_factory,\n",
        "    validation=validation_factory,\n",
        "    training_kwargs=dict(num_epochs=150),\n",
        "    model_kwargs=dict(embedding_dim=200),\n",
        "    optimizer='adam',\n",
        "    optimizer_kwargs=dict(lr=1e-3, weight_decay=1e-5),\n",
        "    random_seed=42,\n",
        "    device='cpu'\n",
        ")"
      ],
      "metadata": {
        "id": "NL6VHkJPq53d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr2DsStd3BJV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = result.model\n",
        "losses = getattr(result, 'losses', None) or result.training_loop.losses\n",
        "for epoch, loss in enumerate(losses):\n",
        "    print(f'Epoch {epoch}: {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Hj4x-l2AAE"
      },
      "outputs": [],
      "source": [
        "from pykeen.evaluation import RankBasedEvaluator\n",
        "\n",
        "evaluator = RankBasedEvaluator()\n",
        "metrics = evaluator.evaluate(\n",
        "    model=model,\n",
        "    mapped_triples=testing_factory.mapped_triples,\n",
        "    additional_filter_triples=[\n",
        "        training_factory.mapped_triples,\n",
        "        validation_factory.mapped_triples\n",
        "    ])\n",
        "metrics_dict = metrics.to_dict()\n",
        "print(metrics_dict)\n",
        "#model still struggling to make correct ranks at top even though loss is low/so model is probably overfit\n",
        "#model basically just memorized the data\n",
        "#need to push embeddings further away from eachother"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "116aa713"
      },
      "source": [
        "Here are the current evaluation metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09485564",
        "collapsed": true
      },
      "source": [
        "import json\n",
        "\n",
        "print(json.dumps(metrics_dict, indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b0oWUfX3JY3"
      },
      "source": [
        "{'head': {'optimistic': {'adjusted_geometric_mean_rank_index': 0.16580402358114887, 'inverse_geometric_mean_rank': 0.004612847666250494, 'adjusted_arithmetic_mean_rank_index': 0.10241403011809802, 'median_absolute_deviation': 243.14676383491872, 'adjusted_arithmetic_mean_rank': 0.8978794990550167, 'variance': 37209.15605965133, 'inverse_harmonic_mean_rank': 0.023817669686371955, 'geometric_mean_rank': 216.7858278339462, 'arithmetic_mean_rank': 313.27536231884056, 'z_geometric_mean_rank': 1.4127554903128567, 'standard_deviation': 192.89674973843216, 'harmonic_mean_rank': 41.98563558769069, 'z_inverse_harmonic_mean_rank': 2.378277700601763, 'inverse_median_rank': 0.0034965034965034965, 'adjusted_inverse_harmonic_mean_rank': 0.013733658143838551, 'count': 69.0, 'median_rank': 286.0, 'inverse_arithmetic_mean_rank': 0.0031920799407846043, 'z_arithmetic_mean_rank': 1.471361261261439, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.028985507246376812, 'hits_at_10': 0.028985507246376812, 'z_hits_at_k': 1.0220937438451763, 'adjusted_hits_at_k': 0.014847366942486626}, 'realistic': {'adjusted_geometric_mean_rank_index': 0.16580431928466743, 'inverse_geometric_mean_rank': 0.004612849093973637, 'adjusted_arithmetic_mean_rank_index': 0.10241403647447012, 'median_absolute_deviation': 243.1467742919922, 'adjusted_arithmetic_mean_rank': 0.8978794927168626, 'variance': 37209.1640625, 'inverse_harmonic_mean_rank': 0.023817671462893486, 'geometric_mean_rank': 216.78575134277344, 'arithmetic_mean_rank': 313.2753601074219, 'z_geometric_mean_rank': 1.4127580098944712, 'standard_deviation': 192.8967742919922, 'harmonic_mean_rank': 41.985633850097656, 'z_inverse_harmonic_mean_rank': 2.378278011422565, 'inverse_median_rank': 0.003496503457427025, 'adjusted_inverse_harmonic_mean_rank': 0.013733659938711638, 'count': 69.0, 'median_rank': 286.0, 'inverse_arithmetic_mean_rank': 0.0031920799519866705, 'z_arithmetic_mean_rank': 1.471361352582127, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.028985507246376812, 'hits_at_10': 0.028985507246376812, 'z_hits_at_k': 1.0220937438451763, 'adjusted_hits_at_k': 0.014847366942486626}, 'pessimistic': {'adjusted_geometric_mean_rank_index': 0.16580402358114887, 'inverse_geometric_mean_rank': 0.004612847666250494, 'adjusted_arithmetic_mean_rank_index': 0.10241403011809802, 'median_absolute_deviation': 243.14676383491872, 'adjusted_arithmetic_mean_rank': 0.8978794990550167, 'variance': 37209.15605965133, 'inverse_harmonic_mean_rank': 0.023817669686371955, 'geometric_mean_rank': 216.7858278339462, 'arithmetic_mean_rank': 313.27536231884056, 'z_geometric_mean_rank': 1.4127554903128567, 'standard_deviation': 192.89674973843216, 'harmonic_mean_rank': 41.98563558769069, 'z_inverse_harmonic_mean_rank': 2.378277700601763, 'inverse_median_rank': 0.0034965034965034965, 'adjusted_inverse_harmonic_mean_rank': 0.013733658143838551, 'count': 69.0, 'median_rank': 286.0, 'inverse_arithmetic_mean_rank': 0.0031920799407846043, 'z_arithmetic_mean_rank': 1.471361261261439, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.028985507246376812, 'hits_at_10': 0.028985507246376812, 'z_hits_at_k': 1.0220937438451763, 'adjusted_hits_at_k': 0.014847366942486626}}, 'tail': {'optimistic': {'adjusted_geometric_mean_rank_index': 0.23783993154184757, 'inverse_geometric_mean_rank': 0.005041923542411874, 'adjusted_arithmetic_mean_rank_index': 0.11178624937572834, 'median_absolute_deviation': 235.73375274239072, 'adjusted_arithmetic_mean_rank': 0.8885338423870192, 'variance': 42690.936357908, 'inverse_harmonic_mean_rank': 0.02557077187975447, 'geometric_mean_rank': 198.33700205648816, 'arithmetic_mean_rank': 310.30434782608694, 'z_geometric_mean_rank': 2.0265277711959815, 'standard_deviation': 206.61785101464008, 'harmonic_mean_rank': 39.107149549589664, 'z_inverse_harmonic_mean_rank': 2.687674542771111, 'inverse_median_rank': 0.0033222591362126247, 'adjusted_inverse_harmonic_mean_rank': 0.015513110770625681, 'count': 69.0, 'median_rank': 301.0, 'inverse_arithmetic_mean_rank': 0.003222642566904862, 'z_arithmetic_mean_rank': 1.6060185698227158, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.014492753623188406, 'hits_at_10': 0.043478260869565216, 'z_hits_at_k': 2.0361939133996865, 'adjusted_hits_at_k': 0.029564463411040368}, 'realistic': {'adjusted_geometric_mean_rank_index': 0.23780891909734225, 'inverse_geometric_mean_rank': 0.005041719414293766, 'adjusted_arithmetic_mean_rank_index': 0.11176546826147482, 'median_absolute_deviation': 235.7337646484375, 'adjusted_arithmetic_mean_rank': 0.8885545639960694, 'variance': 42689.05078125, 'inverse_harmonic_mean_rank': 0.025570547208189964, 'geometric_mean_rank': 198.34503173828125, 'arithmetic_mean_rank': 310.31158447265625, 'z_geometric_mean_rank': 2.0262635280151358, 'standard_deviation': 206.61328125, 'harmonic_mean_rank': 39.10749435424805, 'z_inverse_harmonic_mean_rank': 2.6876352162501185, 'inverse_median_rank': 0.003322259057313204, 'adjusted_inverse_harmonic_mean_rank': 0.015512883780092909, 'count': 69.0, 'median_rank': 301.0, 'inverse_arithmetic_mean_rank': 0.003222567494958639, 'z_arithmetic_mean_rank': 1.6057200102451379, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.014492753623188406, 'hits_at_10': 0.043478260869565216, 'z_hits_at_k': 2.0361939133996865, 'adjusted_hits_at_k': 0.029564463411040368}, 'pessimistic': {'adjusted_geometric_mean_rank_index': 0.23777842325107368, 'inverse_geometric_mean_rank': 0.00504151873017298, 'adjusted_arithmetic_mean_rank_index': 0.11174463126352585, 'median_absolute_deviation': 235.73375274239072, 'adjusted_arithmetic_mean_rank': 0.8885753413287961, 'variance': 42687.17370300357, 'inverse_harmonic_mean_rank': 0.02557032704385443, 'geometric_mean_rank': 198.352927663464, 'arithmetic_mean_rank': 310.3188405797101, 'z_geometric_mean_rank': 2.0260036865370097, 'standard_deviation': 206.60874546592547, 'harmonic_mean_rank': 39.10782987972537, 'z_inverse_harmonic_mean_rank': 2.6875966786746934, 'inverse_median_rank': 0.0033222591362126247, 'adjusted_inverse_harmonic_mean_rank': 0.01551266134331089, 'count': 69.0, 'median_rank': 301.0, 'inverse_arithmetic_mean_rank': 0.0032224920605268075, 'z_arithmetic_mean_rank': 1.6054206477937427, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.014492753623188406, 'hits_at_10': 0.043478260869565216, 'z_hits_at_k': 2.0361939133996865, 'adjusted_hits_at_k': 0.029564463411040368}}, 'both': {'optimistic': {'adjusted_geometric_mean_rank_index': 0.19987475868254778, 'inverse_geometric_mean_rank': 0.004822616016855171, 'adjusted_arithmetic_mean_rank_index': 0.10710233482881726, 'median_absolute_deviation': 246.8532693811827, 'adjusted_arithmetic_mean_rank': 0.8932044881309488, 'variance': 39952.252940558705, 'inverse_harmonic_mean_rank': 0.02469422078306321, 'geometric_mean_rank': 207.3563386562342, 'arithmetic_mean_rank': 311.78985507246375, 'z_geometric_mean_rank': 2.3970850283381324, 'standard_deviation': 199.88059670853173, 'harmonic_mean_rank': 40.49530490493794, 'z_inverse_harmonic_mean_rank': 3.5821192660106953, 'inverse_median_rank': 0.0034662045060658577, 'adjusted_inverse_harmonic_mean_rank': 0.014623388181013136, 'count': 138.0, 'median_rank': 288.5, 'inverse_arithmetic_mean_rank': 0.0032072884467892254, 'z_arithmetic_mean_rank': 2.176080258766568, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.021739130434782608, 'hits_at_10': 0.036231884057971016, 'z_hits_at_k': 2.1623692158655206, 'adjusted_hits_at_k': 0.022205965699469914}, 'realistic': {'adjusted_geometric_mean_rank_index': 0.1998586665765506, 'inverse_geometric_mean_rank': 0.0048225196078419685, 'adjusted_arithmetic_mean_rank_index': 0.10709189874278813, 'median_absolute_deviation': 246.853271484375, 'adjusted_arithmetic_mean_rank': 0.8932148943200499, 'variance': 39951.296875, 'inverse_harmonic_mean_rank': 0.024694109335541725, 'geometric_mean_rank': 207.36048889160156, 'arithmetic_mean_rank': 311.7934875488281, 'z_geometric_mean_rank': 2.39689203675373, 'standard_deviation': 199.87820434570312, 'harmonic_mean_rank': 40.495487213134766, 'z_inverse_harmonic_mean_rank': 3.5820916841284673, 'inverse_median_rank': 0.003466204507276416, 'adjusted_inverse_harmonic_mean_rank': 0.014623275582704525, 'count': 138.0, 'median_rank': 288.5, 'inverse_arithmetic_mean_rank': 0.0032072511967271566, 'z_arithmetic_mean_rank': 2.175868220804716, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.021739130434782608, 'hits_at_10': 0.036231884057971016, 'z_hits_at_k': 2.1623692158655206, 'adjusted_hits_at_k': 0.022205965699469914}, 'pessimistic': {'adjusted_geometric_mean_rank_index': 0.19984248035612395, 'inverse_geometric_mean_rank': 0.004822422410867445, 'adjusted_arithmetic_mean_rank_index': 0.10708151602527383, 'median_absolute_deviation': 246.8532693811827, 'adjusted_arithmetic_mean_rank': 0.8932252472935241, 'variance': 39950.35013652594, 'inverse_harmonic_mean_rank': 0.02469399836511319, 'geometric_mean_rank': 207.36466339955535, 'arithmetic_mean_rank': 311.7971014492754, 'z_geometric_mean_rank': 2.3966979164610738, 'standard_deviation': 199.87583680006432, 'harmonic_mean_rank': 40.495669644684384, 'z_inverse_harmonic_mean_rank': 3.5820642203208393, 'inverse_median_rank': 0.0034662045060658577, 'adjusted_inverse_harmonic_mean_rank': 0.014623163466415245, 'count': 138.0, 'median_rank': 288.5, 'inverse_arithmetic_mean_rank': 0.0032072139072232032, 'z_arithmetic_mean_rank': 2.1756572671719, 'hits_at_1': 0.014492753623188406, 'hits_at_3': 0.014492753623188406, 'hits_at_5': 0.021739130434782608, 'hits_at_10': 0.036231884057971016, 'z_hits_at_k': 2.1623692158655206, 'adjusted_hits_at_k': 0.022205965699469914}}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6mgkdHJ3wWa"
      },
      "outputs": [],
      "source": [
        "from pykeen.predict import predict_triples, predict_target, predict_all\n",
        "\n",
        "# Check for the correct key for 'Gut microbiota' in the training factory\n",
        "head = None\n",
        "if 'Gut microbiota' in training_factory.entity_to_id:\n",
        "    head = 'Gut microbiota'\n",
        "elif 'wd:Q739734' in training_factory.entity_to_id:\n",
        "    head = 'wd:Q739734'\n",
        "\n",
        "if head:\n",
        "    relation = 'indicate' # Assuming 'indicate' is a valid relation in your data\n",
        "    tail_scores = predict_target(model=model, head=head, relation=relation, triples_factory=result.training)\n",
        "\n",
        "    df = tail_scores.df\n",
        "\n",
        "    print(df.nlargest(5, columns='score'))\n",
        "\n",
        "    #Future: Add more papers on parkisons specifically\n",
        "    #Getting better answers at the top, improving model accuracy as it might overfit right now\n",
        "    #Also: getting more specific predictions in data\n",
        "else:\n",
        "    print(\"Could not find 'Gut microbiota' or its QID in the entity mapping.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pykeen.predict import predict_triples, predict_target, predict_all\n",
        "\n",
        "# Check for the correct key for 'Gut microbiota' in the training factory\n",
        "head = None\n",
        "if 'synbiotics' in training_factory.entity_to_id:\n",
        "    head = 'synbiotics'\n",
        "elif 'wd:Q2375654' in training_factory.entity_to_id:\n",
        "    head = 'wd:Q2375654'\n",
        "\n",
        "if head:\n",
        "    relation = 'influence' # Assuming 'indicate' is a valid relation in your data\n",
        "    tail_scores = predict_target(model=model, head=head, relation=relation, triples_factory=result.training)\n",
        "\n",
        "    df = tail_scores.df\n",
        "\n",
        "    print(df.nlargest(5, columns='score'))\n",
        "\n",
        "    #Future: Add more papers on parkisons specifically\n",
        "    #Getting better answers at the top, improving model accuracy as it might overfit right now\n",
        "    #Also: getting more specific predictions in data\n",
        "else:\n",
        "    print(\"Could not find 'Gut microbiota' or its QID in the entity mapping.\")"
      ],
      "metadata": {
        "id": "ZC_CfB4Ilsgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev7xCl1R32i1"
      },
      "outputs": [],
      "source": [
        "entity_embeddings = result.model.entity_representations[0](indices=None)\n",
        "print(entity_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm3b0nWX8Z_5"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "embeddings_np = entity_embeddings.detach().numpy()\n",
        "\n",
        "# Convert complex embeddings to real-valued (magnitude)\n",
        "embeddings_real = np.abs(embeddings_np)\n",
        "\n",
        "tsne = TSNE(n_components=2,perplexity=5, random_state=42)\n",
        "embeddings_2d = TSNE(n_components=2).fit_transform(embeddings_real)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "model = result.model\n",
        "tf = training_factory  # or all_triples_factory if you want the full vocabulary\n",
        "\n",
        "# --- robust ways to get embeddings (works across most PyKEEN versions) ---\n",
        "rep = model.entity_representations[0]\n",
        "try:\n",
        "    E = rep.get_in_canonical_shape().detach().cpu().numpy()\n",
        "except AttributeError:\n",
        "    # fallback: many reps act like nn.Embedding; passing None returns full table\n",
        "    E = rep(indices=None).detach().cpu().numpy()\n",
        "\n",
        "labels = [tf.entity_id_to_label[i] for i in range(E.shape[0])]\n",
        "print(E.shape, \"embeddings loaded\")"
      ],
      "metadata": {
        "id": "4PD0Ef_WC57g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XY = None\n",
        "try:\n",
        "    import umap\n",
        "    reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, random_state=42)\n",
        "    # Convert complex embeddings to real-valued (magnitude) before UMAP\n",
        "    E_real = np.abs(E)\n",
        "\n",
        "    print(f\"NaNs before filtering: {np.isnan(E_real).sum()}\")\n",
        "    print(f\"Infs before filtering: {np.isinf(E_real).sum()}\")\n",
        "\n",
        "    # Filter out rows with NaN or inf values\n",
        "    finite_mask = np.all(np.isfinite(E_real), axis=1)\n",
        "    if not np.all(finite_mask):\n",
        "        print(\"Warning: Embeddings contain NaN or Inf values. Removing affected entities.\")\n",
        "        E_real = E_real[finite_mask]\n",
        "        # Update labels to match the filtered embeddings\n",
        "        labels = [labels[i] for i, mask_val in enumerate(finite_mask) if mask_val]\n",
        "\n",
        "    print(f\"NaNs after filtering: {np.isnan(E_real).sum()}\")\n",
        "    print(f\"Infs after filtering: {np.isinf(E_real).sum()}\")\n",
        "\n",
        "    # Check shape and dtype before UMAP\n",
        "    print(f\"Shape of E_real before UMAP: {E_real.shape}\")\n",
        "    print(f\"Dtype of E_real before UMAP: {E_real.dtype}\")\n",
        "\n",
        "    # Final check for NaN or inf after filtering\n",
        "    if np.isnan(E_real).any() or np.isinf(E_real).any():\n",
        "        print(\"Error: NaN or Inf values still present after filtering.\")\n",
        "    else:\n",
        "        XY = reducer.fit_transform(E_real)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"UMAP failed: {e}\")\n",
        "    try:\n",
        "        from sklearn.manifold import TSNE\n",
        "        reducer = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42)\n",
        "        # Convert complex embeddings to real-valued (magnitude) before TSNE\n",
        "        E_real = np.abs(E)\n",
        "\n",
        "        print(f\"NaNs before filtering (TSNE): {np.isnan(E_real).sum()}\")\n",
        "        print(f\"Infs before filtering (TSNE): {np.isinf(E_real).sum()}\")\n",
        "\n",
        "        # Filter out rows with NaN or inf values\n",
        "        finite_mask = np.all(np.isfinite(E_real), axis=1)\n",
        "        if not np.all(finite_mask):\n",
        "            print(\"Warning: Embeddings contain NaN or Inf values. Removing affected entities.\")\n",
        "            E_real = E_real[finite_mask]\n",
        "            # Update labels to match the filtered embeddings\n",
        "            labels = [labels[i] for i, mask_val in enumerate(finite_mask) if mask_val]\n",
        "\n",
        "        print(f\"NaNs after filtering (TSNE): {np.isnan(E_real).sum()}\")\n",
        "        print(f\"Infs after filtering (TSNE): {np.isinf(E_real).sum()}\")\n",
        "\n",
        "        # Check shape and dtype before TSNE\n",
        "        print(f\"Shape of E_real before TSNE: {E_real.shape}\")\n",
        "        print(f\"Dtype of E_real before TSNE: {E_real.dtype}\")\n",
        "\n",
        "        # Final check for NaN or inf after filtering\n",
        "        if np.isnan(E_real).any() or np.isinf(E_real).any():\n",
        "            print(\"Error: NaN or Inf values still present after filtering.\")\n",
        "        else:\n",
        "            XY = reducer.fit_transform(E_real)\n",
        "\n",
        "    except Exception as e_tsne:\n",
        "        print(f\"TSNE failed: {e_tsne}\")\n",
        "        XY = None\n",
        "\n",
        "# Re-build qid2label with *bare* QIDs using the original label2qid and the filtered labels\n",
        "qid_to_original_label = {}\n",
        "for label_string in labels:\n",
        "    original_label = label2qid.get(label_string)\n",
        "    if original_label and isinstance(original_label, str) and (original_label.startswith('wd:Q') or original_label.startswith('Q')):\n",
        "        bare_qid = original_label.replace('wd:', '')\n",
        "        qid_to_original_label[bare_qid] = label_string"
      ],
      "metadata": {
        "id": "n2GybfBLDRF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA699njs8e9z",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install adjustText\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# build qid2label with *bare* QIDs\n",
        "qid2label = { (v[3:] if v.startswith('wd:') else v): k\n",
        "              for k, v in label2qid.items() if v }\n",
        "\n",
        "def prettify(lbl: str) -> str:\n",
        "    # normalize label to a bare QID if it’s a QID-form label\n",
        "    q = None\n",
        "    if lbl.startswith('wd:'):\n",
        "        q = lbl[3:]\n",
        "    elif lbl.startswith('Q'):\n",
        "        q = lbl\n",
        "\n",
        "    if q and q in qid2label and qid2label[q]:\n",
        "        return qid2label[q]          # human-readable name from cache\n",
        "    return lbl                       # always fall back to original label\n",
        "\n",
        "pretty_labels = [prettify(x) for x in labels]\n",
        "# Get entity labels from the training dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# simple coloring rule: QIDs vs raw strings\n",
        "colors = ['C0' if s.startswith('wd:') else 'C1' for s in labels]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan\n",
        "\n",
        "\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Create a mapping from QID string (bare) to original label string using the filtered labels\n",
        "qid_to_original_label = {}\n",
        "if 'labels' in locals(): # Check if labels variable exists (from n2GybfBLDRF3)\n",
        "   for label_string in labels:\n",
        "       original_label = label2qid.get(label_string)\n",
        "       if original_label and isinstance(original_label, str) and (original_label.startswith('wd:Q') or original_label.startswith('Q')):\n",
        "           bare_qid = original_label.replace('wd:', '')\n",
        "           qid_to_original_label[bare_qid] = label_string\n",
        "\n",
        "\n",
        "def get_display_label(filtered_entity_index: int) -> str:\n",
        "   \"\"\"Gets the label string from the filtered labels list and attempts to find the original label.\"\"\"\n",
        "   label_string = labels[filtered_entity_index]\n",
        "\n",
        "\n",
        "   # If the label string is a QID, try to find the original label using the updated mapping\n",
        "   if isinstance(label_string, str) and (label_string.startswith('wd:Q') or label_string.startswith('Q')):\n",
        "       bare_qid = label_string.replace('wd:', '')\n",
        "       original_label = qid_to_original_label.get(bare_qid)\n",
        "       if original_label:\n",
        "           return f\"{original_label} ({label_string})\" # Show both if original label found\n",
        "       else:\n",
        "           return label_string # Otherwise, show the QID string\n",
        "\n",
        "\n",
        "   return label_string # Return the label string if it's not a QID\n",
        "\n",
        "\n",
        "if XY is None:\n",
        "   print(\"Dimensionality reduction failed. Cannot perform clustering or plot.\")\n",
        "else:\n",
        "   # cluster directly in 2-D space\n",
        "   clusterer = hdbscan.HDBSCAN(min_cluster_size=8, min_samples=4)\n",
        "   labels_hdb = clusterer.fit_predict(XY)\n",
        "\n",
        "\n",
        "   import numpy as np\n",
        "\n",
        "\n",
        "   unique, counts = np.unique(labels_hdb, return_counts=True)\n",
        "   print(dict(zip(unique, counts)))\n",
        "\n",
        "\n",
        "   n_clusters = len(set(labels_hdb)) - (1 if -1 in labels_hdb else 0)\n",
        "   print(\"Number of clusters:\", n_clusters)\n",
        "\n",
        "\n",
        "   for cid in np.unique(labels_hdb):\n",
        "       if cid == -1:\n",
        "           continue\n",
        "       members = np.where(labels_hdb == cid)[0]\n",
        "       print(f\"\\nCluster {cid} ({len(members)} concepts):\")\n",
        "       # Use the new get_display_label function to print labels\n",
        "       print([get_display_label(members[i]) for i in range(min(10, len(members)))])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   plt.figure(figsize=(9,7))\n",
        "   plt.scatter(XY[:,0], XY[:,1],\n",
        "               c=labels_hdb,\n",
        "               cmap='Spectral', s=8, alpha=0.7)\n",
        "   plt.title(\"Concept Embedding Map - colored by HDBSCAN clusters\")\n",
        "   plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
        "   plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "J0qHPJuygzrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "unique, counts = np.unique(labels_hdb, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "n_clusters = len(set(labels_hdb)) - (1 if -1 in labels_hdb else 0)\n",
        "print(\"Number of clusters:\", n_clusters)"
      ],
      "metadata": {
        "id": "L3vdnG5SKnBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start from your existing mapping: label2qid = {\"Lactobacillus\":\"Q131647\", ...}\n",
        "qid2label = {}\n",
        "for label, q in label2qid.items():\n",
        "    if q:\n",
        "        qid = q[3:] if q.startswith('wd:') else q\n",
        "        qid2label[qid] = label\n",
        "import re\n",
        "\n",
        "def extract_qid(s):\n",
        "    if isinstance(s, str):\n",
        "        m = re.search(r'(?:wd:)?(Q\\d+)$', s.strip())\n",
        "        if m: return m.group(1)\n",
        "    return None\n",
        "\n",
        "# flatten your cluster terms (whatever structure you have)\n",
        "all_terms = []\n",
        "# Check if labels_hdb exists and is not None before iterating\n",
        "if 'labels_hdb' in locals() and labels_hdb is not None:\n",
        "    for cid in sorted(set(labels_hdb) - {-1}):\n",
        "        members = (labels_hdb == cid).nonzero()[0]\n",
        "        all_terms.extend([labels[i] for i in members])\n",
        "else:\n",
        "    # If labels_hdb doesn't exist or is None, use all original labels\n",
        "    all_terms = labels\n",
        "\n",
        "\n",
        "missing_qids = sorted({extract_qid(t) for t in all_terms if extract_qid(t)} - set(qid2label))\n",
        "print(f\"Missing QIDs to resolve: {len(missing_qids)}\")\n",
        "\n",
        "import requests, time\n",
        "\n",
        "def fetch_wikidata_labels(qids, language='en', batch=50, pause=0.5): # Increased pause\n",
        "    \"\"\"Return { 'Q123': 'Human-readable label', ... } for the given QIDs.\"\"\"\n",
        "    out = {}\n",
        "    for i in range(0, len(qids), batch):\n",
        "        chunk = qids[i:i+batch]\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'format': 'json',\n",
        "            'ids': '|'.join(chunk),\n",
        "            'props': 'labels',\n",
        "            'languages': language,\n",
        "            'languagefallback': 1,\n",
        "        }\n",
        "        try:\n",
        "            r = requests.get('https://www.wikidata.org/w/api.php', params=params, headers={\"User-Agent\": \"LailahKG/1.0 (mailto:your_email@example.com)\"}, timeout=30) # Added User-Agent\n",
        "            r.raise_for_status()\n",
        "            data = r.json().get('entities', {})\n",
        "            for qid, obj in data.items():\n",
        "                lab = obj.get('labels', {}).get(language, {}).get('value')\n",
        "                if lab:\n",
        "                    out[qid] = lab\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching batch starting with {chunk[0]}: {e}\")\n",
        "            # Continue with the next batch\n",
        "            pass\n",
        "        time.sleep(pause)  # be polite to the API\n",
        "    return out\n",
        "\n",
        "# fetch & merge\n",
        "fetched = fetch_wikidata_labels(missing_qids, language='en')\n",
        "qid2label.update(fetched)\n",
        "\n",
        "print(f\"Fetched {len(fetched)} new labels.\")\n",
        "def qid_to_label_or_self(x):\n",
        "    qid = extract_qid(x)\n",
        "    if qid and qid in qid2label:\n",
        "        return qid2label[qid]\n",
        "    return x  # fallback\n",
        "\n",
        "def pretty_list(terms, max_items=10):\n",
        "    return [qid_to_label_or_self(t) for t in terms[:max_items]]\n",
        "\n",
        "# Check if labels_hdb exists and is not None before iterating\n",
        "if 'labels_hdb' in locals() and labels_hdb is not None:\n",
        "    for cid in sorted(set(labels_hdb) - {-1}):\n",
        "        members = (labels_hdb == cid).nonzero()[0]\n",
        "        terms = [labels[i] for i in members]\n",
        "        print(f\"\\nCluster {cid} ({len(members)} concepts):\")\n",
        "        print(pretty_list(terms, max_items=12))\n",
        "\n",
        "    pretty_labels = [qid_to_label_or_self(x) for x in labels]\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    from adjustText import adjust_text # Import adjustText\n",
        "\n",
        "    plt.figure(figsize=(9,7))\n",
        "    plt.scatter(XY[:,0], XY[:,1], c=labels_hdb, cmap='Spectral', s=8, alpha=0.75)\n",
        "    plt.title(\"Concept Embedding Map — HDBSCAN clusters (human labels)\")\n",
        "    plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
        "\n",
        "    # Check if XY exists before attempting to annotate\n",
        "    if XY is not None:\n",
        "        texts = [] # Create a list to hold the text annotations\n",
        "        for cid in sorted(set(labels_hdb) - {-1}):\n",
        "            members = (labels_hdb == cid).nonzero()[0]\n",
        "            if len(members) == 0:\n",
        "                continue\n",
        "            center = XY[members].mean(axis=0)\n",
        "            d = ((XY[members]-center)**2).sum(axis=1)\n",
        "            rep = members[np.argmin(d)]\n",
        "            # txt = pretty_labels[rep] # Remove entity label\n",
        "            # txt = txt if isinstance(txt, str) else \"\" # Remove entity label\n",
        "            txt = f\"Cluster {cid}\" # Only include cluster number\n",
        "            texts.append(plt.text(XY[rep,0], XY[rep,1], txt, fontsize=9, weight='bold')) # Append text annotation\n",
        "\n",
        "        # Use adjust_text to prevent overlaps\n",
        "        adjust_text(\n",
        "            texts,\n",
        "            expand_points=(1, 1), # Increase this value to push labels further apart\n",
        "            force_points=(0.5, 0.5), # Increase this value to force points apart more strongly\n",
        "            force_text=(0.5, 0.5), # Increase this value to force text apart more strongly\n",
        "            arrowprops=dict(arrowstyle='-', color='k', lw=0.5, alpha=0.6)\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Clustering results (labels_hdb) not available. Cannot print cluster details or plot.\")"
      ],
      "metadata": {
        "id": "tqqVuxh_R7i5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}